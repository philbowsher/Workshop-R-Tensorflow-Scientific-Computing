---
title: "Hello Keras - A Simple Artificial Neural Network using Keras"
output: github_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(265509)
```

## Task

_Copy relevant code from below to [R/02_hello_keras.R](https://github.com/leonjessen/RPharma2019/blob/master/R/02_hello_keras.R) and create a working model_

Note how this is now a 3-class classifier and we are evaluating the predictive performance of the model on left out data.

### Introduction

The aim of this exercise is to let you build your first artificial neural network using Keras. Naturally, we will use the `iris` data set.

### Load libraries
```{r message=FALSE}
library('tidyverse')
library('keras')
```

### Data
[The famous Iris flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) contains data to quantify the morphologic variation of Iris flowers of three related species. In other words - A total of 150 observations of 4 input features `Sepal.Length`, `Sepal.Width`, `Petal.Length` and `Petal.Width` and 3 output classes `setosa` `versicolor` and `virginica`, with 50 observations in each class:

```{r see_iris}
head(iris)
```

### Aim
Our aim is to create a model, which connect the 4 input features (`Sepal.Length`, `Sepal.Width`, `Petal.Length` and `Petal.Width`) to the correct output class (`setosa` `versicolor` and `virginica`) using an artificial neural network. For this task, we have chosen the following simple architecture with one input layer with 4 neurons (one for each feature), one hidden layer with 4 neurons and one output layer with 3 neurons (one for each class), all fully connected:

```{r see_architecture, fig.align="center", out.width = "500px", echo=FALSE}
#knitr::include_graphics("img/architecture_visualisation.png")
```

![](https://raw.githubusercontent.com/leonjessen/keras_tensorflow_on_iris/master/img/architecture_visualisation.png)

Our artificial neural network will have a total of 35 parameters: 4 for each input neuron connected to the hidden layer, plus an additional 4 for the associated first bias neuron and 3 for each of the hidden neurons connected to the output layer, plus an additional 3 for the associated second bias neuron. I.e. 4 x 4 + 4 + 4 x 3 + 3 = 35

### 1. Prepare data

We start with slightly wrangling the iris data set by renaming the input features and converting character labels to numeric:
```{r wrangle_iris}
nn_dat = iris %>% as_tibble %>%
  rename(sepal_l_feat = Sepal.Length,
         sepal_w_feat = Sepal.Width,
         petal_l_feat = Petal.Length,
         petal_w_feat = Petal.Width) %>%
  mutate(class_num = as.numeric(Species) - 1, # factor, so = 0, 1, 2
         class_label = Species)
nn_dat %>% head(3)
```

Then, we split the iris data into a training and a test data set, setting aside 20% of the data for left out data partition, to be used for final performance evaluation:
```{r create_test_and_train}
test_f = 0.20
nn_dat = nn_dat %>%
  mutate(partition = sample(x = c('train','test'),
                            size = nrow(.),
                            replace = TRUE,
                            prob = c(1 - test_f, test_f)))
nn_dat %>% count(partition)
```

Based on the partition, we can now create training and test data
```{r set_data}
x_train = nn_dat %>%
  filter(partition == 'train') %>%
  select(contains("feat")) %>%
  as.matrix
y_train = nn_dat %>%
  filter(partition == 'train') %>%
  pull(class_num) %>%
  to_categorical(3)

x_test = nn_dat %>%
  filter(partition == 'test') %>%
  select(contains("feat")) %>%
  as.matrix
y_test = nn_dat %>%
  filter(partition == 'test') %>%
  pull(class_num) %>%
  to_categorical(3)
```

### 2. Define model

Set architecture (See the green ANN visualisation)
```{r set_architecture}
model = keras_model_sequential() %>% 
  layer_dense(units = 4, activation = 'relu', input_shape = 4) %>% 
  layer_dense(units = 3, activation = 'softmax')
```

Compile model
```{r compile_model}
model %>%
  compile(loss = 'categorical_crossentropy',
          optimizer = optimizer_rmsprop(),
          metrics = c('accuracy')
)
```

We can get a summary of the model like so:
```{r see_model_summary}
model %>%
  summary
```

As expected we see 35 trainable parameters.

### Train the Artificial Neural Network

Lastly we fit the model and save the training progres in the `history` object:
```{r fit_model}
history = model %>%
  fit(x = x_train,
      y = y_train,
      epochs = 200,
      batch_size = 20,
      validation_split = 0
)
```

Once the model is trained, we can inspect the training process
```{r see_training, fig.align="center", fig.width=10, fig.height=3}
plot(history)
```

### Evaluate Network Performance

The final performance can be obtained like so:
```{r get_performance}
perf = model %>% evaluate(x_test, y_test)
perf
```

Then we can augment the `nn_dat` for plotting:
```{r mk_plot_dat}
plot_dat = nn_dat %>%
  filter(partition == 'test') %>%
  mutate(class_num = factor(class_num),
  y_pred = factor(as.integer(predict(model, x_test) %>% k_argmax())),
         Correct = factor(ifelse(class_num == y_pred, "Yes", "No")))
plot_dat %>% select(-contains("feat")) %>% head(3)
```

and lastly, we can visualise the confusion matrix like so:
```{r conf_mat_vis, fig.align="center", fig.width=10, fig.height=3, echo=FALSE}
title     = "Classification Performance of Artificial Neural Network"
sub_title = str_c("Accuracy = ", round(perf["acc"], 3) * 100, "%")
x_lab     = "True iris class"
y_lab     = "Predicted iris class"
plot_dat %>% ggplot(aes(x = class_num, y = y_pred, colour = Correct)) +
  geom_jitter() +
  scale_x_discrete(labels = levels(nn_dat$class_label)) +
  scale_y_discrete(labels = levels(nn_dat$class_label)) +
  theme_bw() +
  labs(title = title, subtitle = sub_title, x = x_lab, y = y_lab)
```

### Conclusion
Here, we created a 3-class predictor with an accuracy of `r round(perf$acc,3)*100`% on a left out data partition. I hope, this illustrates how relatively simple it is to get started with `Keras`.